# Ollama Configuration (local LLM)
# Make sure Ollama is running: ollama serve
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3
